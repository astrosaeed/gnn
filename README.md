#Documentation
This repository is used for reasoning over the output of GNN-based scene graph generation generated by various images.

In the current setup, a complete trial involves robot doing navigation while taking pictures from the environment. Once navigation is done, the images are sent to the model (based in Google Cloud) to extract the relations (in form of csv files). All CSV files then are passed to the SQL database and to the reasoner.

##How to run the system?

To run the system, first make sure the Segway-based robot is operating and localized perfectly. Then, make sure that the parameters in

```
config.py 
```

file are all updated. Also make sure that bwi_common repository is in gnn branch and do the
 
```
$ roscd bwi_kr_execution
$ git stash 
```

Also, make sure the camera is installed and properly working. To test the camera is working properly, you need


Now, in a new terminal, type down:

```
$ roslaunch gnn main.launch
```

And then let the robot navigate either autonomously or using the joystick

```
$ roslaunch bwi_joy bwi_joystick_lunch
```

After navigation, once you are done, please shutdown the gnn main.launch process

###Relation extraction (Google Cloud)

Now, it is the time to send the images to the Google Cloud: To do that, make sure that Google Cloud Virtual Machine (VM) is turned on. If it is on, on a new terminal, ssh to the VM and make sure the data folders and results folder are empty (To avoid confusion with other previous results)    

Next scp the images to the VM and in the VM, run the following command:


```
$ cd KERN/scripts/
$ sh testone.sgls
```

Once the relationship extraction is done, then scp all the outputs to the results folder.

* REMEMBER TO SHUTDOWN THE GOOGLE CLOUD SERVICE, ELSE YOU WILL BE CHARGED MORE.

Now, it's time to create a database. Just run

```
$ python db.py
```

And now to do the reasoning, please go to the reasoning folder by typing:

```
$ cd reasoning
```

